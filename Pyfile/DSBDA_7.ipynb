{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4520cc24-7273-425a-a7c7-bc2ec091e01c",
   "metadata": {},
   "source": [
    "Word Tokenization: This is the first step in any NLP process that uses text data. Tokenization is a mandatory step, which simplifies things for our machine learning model. It is the process of breaking down a piece of text into individual components or smaller units called tokens. The ultimate goal of tokenization is to process the raw text data and create a vocabulary from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae2d92e-1cec-4b05-877a-a4993daf1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP!!']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love NLP!!'\n",
    "tokens = sentence.split(\" \")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58fcb0bd-f78f-4858-a612-b960f5924b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP!!']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love NLP!!'\n",
    "tokens = sentence.split(\" \")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054ed0c-dc11-44b5-a2ed-9740a49b869c",
   "metadata": {},
   "source": [
    "Lower casing: This step reduces complexity. We convert the text data into the same case, preferably lowercase, so that we don't have to work with both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0db609-89b0-4386-9400-94c6a495974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love nlp!!\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love NLP!!'\n",
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cbffc-d72a-4142-a086-0043e4760304",
   "metadata": {},
   "source": [
    "Punctuation removal: In this step, all the punctuations present in the text are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71f7d201-a61a-4cdf-b115-a02b4ba38406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Messi is great player However he is yet to win a world cup\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "text = \"Messi is great player. However, he is yet to win a world cup\"\n",
    "text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "print(text_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188bc88-f24e-4510-adad-2cca1786be40",
   "metadata": {},
   "source": [
    "Stop word removal: The most commonly used words are called stopwords. They contribute very less to the predictions and add very little value analytically. Hence, removing stopwords will make it easier for our models to train the text data. We can use the PorterStemmer in python to remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb34c79b-8148-4abe-9000-d193e04d8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96985093-8013-4e87-8757-9adcf85c8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f3602aa-a158-4272-9f02-74a98a3f6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    "# converts the words in word_tokens to lower case and then checks whether \n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd9e7c7b-2048-4ac1-93f4-d621d2da889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumps', 'happily', 'running', 'happily']\n",
      "Stemmed words: ['run', 'jump', 'happili', 'run', 'happili']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "# Create a Porter Stemmer instance\n",
    "porter_stemmer = PorterStemmer()\n",
    " \n",
    "# Example words for stemming\n",
    "words = [\"running\", \"jumps\", \"happily\", \"running\", \"happily\"]\n",
    " \n",
    "# Apply stemming to each word\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    " \n",
    "# Print the results\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c9ccf-071d-4e59-8dbf-c2349a53d219",
   "metadata": {},
   "source": [
    "Lemmatization: NLTK uses the WordNetLemmatizer for the purpose of lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "241ed106-b1e9-484c-b7e5-eb94e5e62a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fc955-22f1-4d75-a4cb-11bb76522f1b",
   "metadata": {},
   "source": [
    "Hereâ€™s how you can implement TF-IDF in Python using the scikit-learn. The algorithm works as follows:\n",
    "1. Preprocessing: The text data is preprocessed by removing stop words, punctuation, and other non-alphanumeric characters.\n",
    "2. Tokenization: The text is tokenized into individual words.\n",
    "3. Instantiate TfidfVectorizer and fit the corpus\n",
    "4. Transform that corpus to get the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6230a89a-82b8-4a5e-bf61-726bf94b7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: \n",
      "['quick brown fox jumps lazy dog', 'lazy dog likes sleep day', 'brown fox prefers eat cheese', 'red fox jumps brown fox', 'brown dog chases fox']\n",
      "Feature Names:\n",
      " ['brown' 'chases' 'cheese' 'day' 'dog' 'eat' 'fox' 'jumps' 'lazy' 'likes'\n",
      " 'prefers' 'quick' 'red' 'sleep']\n",
      "TF-IDF Matrix:\n",
      " [[0.30620672 0.         0.         0.         0.36399815 0.\n",
      "  0.30620672 0.43850426 0.43850426 0.         0.         0.54351473\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.49389914 0.33077001 0.\n",
      "  0.         0.         0.39847472 0.49389914 0.         0.\n",
      "  0.         0.49389914]\n",
      " [0.29550385 0.         0.52451722 0.         0.         0.52451722\n",
      "  0.29550385 0.         0.         0.         0.52451722 0.\n",
      "  0.         0.        ]\n",
      " [0.31309104 0.         0.         0.         0.         0.\n",
      "  0.62618207 0.44836297 0.         0.         0.         0.\n",
      "  0.55573434 0.        ]\n",
      " [0.39032474 0.69282362 0.         0.         0.46399205 0.\n",
      "  0.39032474 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus of documents\n",
    "corpus = ['The quick brown fox jumps over the lazy dog.',\n",
    "          'The lazy dog likes to sleep all day.',\n",
    "          'The brown fox prefers to eat cheese.',\n",
    "          'The red fox jumps over the brown fox.',\n",
    "          'The brown dog chases the fox'\n",
    "         ]\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and other non-alphanumeric characters\n",
    "    text =  re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Join the words back into a string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess the corpus\n",
    "corpus = [preprocess_text(doc) for doc in corpus]\n",
    "print('Corpus: \\n{}'.format(corpus))\n",
    "\n",
    "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the preprocessed corpus into a TF-IDF matrix\n",
    "tf_idf_matrix = vectorizer.transform(corpus)\n",
    "\n",
    "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(\"TF-IDF Matrix:\\n\",tf_idf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468ce53-b940-4ede-9481-319ea950859b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
